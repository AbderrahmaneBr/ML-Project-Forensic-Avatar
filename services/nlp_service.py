import os
import base64
import httpx
from collections.abc import Generator

import ollama
from openai import OpenAI

from backend.config import OPENAI_API_KEY, GROQ_API_KEY

OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen3:0.6b")
GPT_VISION_MODEL = "gpt-4o"  # Best for vision tasks
GPT_FAST_MODEL = "gpt-4o-mini"
GROQ_MODEL = "llama-3.3-70b-versatile" # Fast and capable model on Groq

# Initialize OpenAI client for vision
_openai_client = None
if OPENAI_API_KEY:
    _openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Initialize Groq client (using OpenAI SDK)
_groq_client = None
if GROQ_API_KEY:
    _groq_client = OpenAI(
        api_key=GROQ_API_KEY,
        base_url="https://api.groq.com/openai/v1"
    )

SYSTEM_PROMPT = """You are a forensic analyst AI. Analyze evidence precisely and concisely.

For images: List objects detected, transcribe any text found, then provide a brief hypothesis.

Keep responses SHORT (under 150 words). Be direct and professional. No dramatic narration."""


def _confidence_label(confidence: float) -> str:
    """Convert confidence score to a label for the LLM."""
    if confidence >= 0.8:
        return "[HIGH]"
    elif confidence >= 0.5:
        return "[MEDIUM]"
    else:
        return "[LOW]"


def generate_hypothesis(
    detected_objects: list[dict],
    extracted_texts: list[dict],
    context: str | None = None,
    model: str = OLLAMA_MODEL
) -> dict:
    """
    Generate a single forensic hypothesis based on detected objects and extracted text.
    Uses Ollama for local LLM inference.

    Returns the strongest hypothesis as a single dict with content and confidence.
    """
    user_prompt = _build_prompt(detected_objects, extracted_texts, context)

    try:
        response = ollama.chat(
            model=model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ]
        )

        content = response["message"]["content"]
        # Clean up the response into a single paragraph
        clean_content = " ".join(content.split())

        return {
            "content": clean_content,
            "confidence": 0.7
        }

    except Exception as e:
        return {
            "content": f"Unable to generate hypothesis: {str(e)}",
            "confidence": 0.0
        }


def _build_prompt(
    detected_objects: list[dict],
    extracted_texts: list[dict],
    context: str | None = None
) -> str:
    """Build the user prompt for hypothesis generation."""
    objects_str = ", ".join([
        f"{_confidence_label(obj['confidence'])} {obj['label']}"
        for obj in detected_objects
    ]) if detected_objects else "No objects detected"

    texts_str = ", ".join([
        f"{_confidence_label(text.get('confidence', 0.7))} \"{text['text']}\""
        for text in extracted_texts
    ]) if extracted_texts else "No text extracted"

    context_str = f"\n\nAdditional Context: {context}" if context else ""

    return f"""Analyze this crime scene evidence and provide a narrated analysis:

Objects at the scene: {objects_str}

Text found at the scene: {texts_str}{context_str}

Provide your strongest hypothesis about what occurred based on this evidence."""


def generate_hypotheses_stream(
    detected_objects: list[dict],
    extracted_texts: list[dict],
    context: str | None = None,
    model: str = OLLAMA_MODEL
) -> Generator[str, None, None]:
    """
    Stream forensic hypotheses token by token.
    Yields each token as it's generated by the LLM.
    """
    user_prompt = _build_prompt(detected_objects, extracted_texts, context)

    try:
        stream = ollama.chat(
            model=model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            stream=True
        )

        for chunk in stream:
            token = chunk["message"]["content"]
            if token:
                yield token

    except Exception as e:
        yield f"[ERROR] Unable to generate hypothesis: {str(e)}"


def generate_hypotheses_stream_openai(
    detected_objects: list[dict],
    extracted_texts: list[dict],
    context: str | None = None
) -> Generator[str, None, None]:
    """
    Stream forensic hypotheses using OpenAI GPT-4o.
    Used for basic pipeline (YOLO+OCR+OpenAI LLM).
    Yields each token as it's generated.
    """
    if not _openai_client:
        yield "[ERROR] OpenAI API key not configured. Cannot generate hypothesis."
        return
    
    user_prompt = _build_prompt(detected_objects, extracted_texts, context)

    try:
        stream = _openai_client.chat.completions.create(
            model=GPT_FAST_MODEL,  # Use faster model for text-based analysis
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=800,  # Reduced for faster response
            stream=True,
        )

        for chunk in stream:
            token = chunk.choices[0].delta.content
            if token:
                yield token

    except Exception as e:
        yield f"[ERROR] OpenAI error: {str(e)}"


def generate_hypotheses_stream_groq(
    detected_objects: list[dict],
    extracted_texts: list[dict],
    context: str | None = None
) -> Generator[str, None, None]:
    """
    Stream forensic hypotheses using Groq (Llama 3).
    Used for basic pipeline.
    Yields each token as it's generated.
    """
    if not _groq_client:
        yield "[ERROR] Groq API key not configured. Cannot generate hypothesis."
        return
    
    user_prompt = _build_prompt(detected_objects, extracted_texts, context)

    try:
        stream = _groq_client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=800,
            stream=True,
        )

        for chunk in stream:
            token = chunk.choices[0].delta.content
            if token:
                yield token

    except Exception as e:
        yield f"[ERROR] Groq error: {str(e)}"


# ==================== GPT-4o Vision Analysis ====================

def download_image_as_base64(image_url: str) -> str:
    """Download an image from URL and return as base64 encoded string."""
    with httpx.Client(timeout=30.0) as client:
        response = client.get(image_url)
        response.raise_for_status()
        return base64.b64encode(response.content).decode("utf-8")


def get_image_mime_type(image_url: str) -> str:
    """Guess MIME type from URL."""
    url_lower = image_url.lower()
    if ".png" in url_lower:
        return "image/png"
    elif ".gif" in url_lower:
        return "image/gif"
    elif ".webp" in url_lower:
        return "image/webp"
    return "image/jpeg"  # Default to JPEG


def analyze_image_with_vision(
    image_url: str,
    context: str | None = None
) -> dict:
    """
    Analyze an image using GPT-4o Vision.
    
    Args:
        image_url: Presigned URL to the image
        context: Optional additional context from user
    
    Returns:
        dict with 'content' and 'confidence' keys
    """
    if not _openai_client:
        return {
            "content": "OpenAI API key not configured. Cannot perform vision analysis.",
            "confidence": 0.0
        }
    
    try:
        # Download and encode image
        base64_image = download_image_as_base64(image_url)
        mime_type = get_image_mime_type(image_url)
        
        # Build user message with image
        user_content = []
        
        if context:
            user_content.append({"type": "text", "text": f"User context: {context}\n\nAnalyze this image:"})
        else:
            user_content.append({"type": "text", "text": "Analyze this crime scene image:"})
        
        user_content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:{mime_type};base64,{base64_image}"
            }
        })
        
        response = _openai_client.chat.completions.create(
            model=GPT_VISION_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_content}
            ],
            max_tokens=1000,
        )
        
        content = response.choices[0].message.content or ""
        return {
            "content": content,
            "confidence": 0.9  # GPT-4o is highly capable
        }
        
    except Exception as e:
        return {
            "content": f"Vision analysis error: {str(e)}",
            "confidence": 0.0
        }


def analyze_image_with_vision_stream(
    image_url: str,
    context: str | None = None
) -> Generator[str, None, None]:
    """
    Stream GPT-4o Vision analysis token by token.
    
    Args:
        image_url: Presigned URL to the image
        context: Optional additional context from user
    
    Yields:
        Tokens as they're generated
    """
    if not _openai_client:
        yield "[ERROR] OpenAI API key not configured. Cannot perform vision analysis."
        return
    
    try:
        # Download and encode image
        base64_image = download_image_as_base64(image_url)
        mime_type = get_image_mime_type(image_url)
        
        # Build user message with image
        user_content = []
        
        if context:
            user_content.append({"type": "text", "text": f"User context: {context}\n\nAnalyze this image:"})
        else:
            user_content.append({"type": "text", "text": "Analyze this crime scene image:"})
        
        user_content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:{mime_type};base64,{base64_image}"
            }
        })
        
        stream = _openai_client.chat.completions.create(
            model=GPT_VISION_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_content}
            ],
            max_tokens=1000,
            stream=True,
        )
        
        for chunk in stream:
            token = chunk.choices[0].delta.content
            if token:
                yield token
                
    except Exception as e:
        yield f"[ERROR] Vision analysis error: {str(e)}"


def analyze_multiple_images_with_vision_stream(
    image_urls: list[str],
    context: str | None = None
) -> Generator[str, None, None]:
    """
    Analyze multiple images with GPT-4o Vision, streaming the response.
    
    Args:
        image_urls: List of presigned URLs to images
        context: Optional additional context from user
    
    Yields:
        Tokens as they're generated
    """
    if not _openai_client:
        yield "[ERROR] OpenAI API key not configured. Cannot perform vision analysis."
        return
    
    try:
        # Build user message with all images
        user_content = []
        
        if context:
            user_content.append({"type": "text", "text": f"User context: {context}\n\nAnalyze these crime scene images:"})
        else:
            user_content.append({"type": "text", "text": "Analyze these crime scene images:"})
        
        # Add each image
        for image_url in image_urls:
            base64_image = download_image_as_base64(image_url)
            mime_type = get_image_mime_type(image_url)
            user_content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:{mime_type};base64,{base64_image}"
                }
            })
        
        stream = _openai_client.chat.completions.create(
            model=GPT_VISION_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_content}
            ],
            max_tokens=1500,
            stream=True,
        )
        
        for chunk in stream:
            token = chunk.choices[0].delta.content
            if token:
                yield token
                
    except Exception as e:
        yield f"[ERROR] Vision analysis error: {str(e)}"
